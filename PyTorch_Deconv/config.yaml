# Optimized configuration for ZS-DeconvNet two-stage deconvolution

# Model configuration
model:
  # Two-stage architecture: denoising + deconvolution (matches TensorFlow implementation)
  architecture: "two_stage"
  
  input_channels: 1
  output_channels: 1
  conv_block_num: 4
  conv_num: 3
  upsample_flag: true
  insert_xy: 16

# Loss function configuration - optimized based on analysis
loss:
  tv_weight: 0.0           # Total variation regularization weight
  hessian_weight: 0.001    # Hessian regularization weight (reduced from 0.01 to prevent over-smoothing)
  l1_weight: 0.0           # L1 regularization weight
  use_mse: false           # Use MSE loss (false = MAE loss)
  upsample_flag: true      # Whether output is upsampled
  insert_xy: 16            # Padding size for cropping
  
  # Two-stage loss weights - rebalanced for better deconvolution performance
  denoise_loss_weight: 0.3  # Weight for denoising loss (stage 1) - reduced to emphasize deconvolution
  deconv_loss_weight: 0.7   # Weight for deconvolution loss (stage 2) - increased for sharper results

# Optimizer configuration
optimizer:
  lr: 0.0001               # Learning rate (1e-04) - increased to help escape local minima
  weight_decay: 0.00001    # Weight decay for regularization (1e-5)

# Learning rate scheduler (optional)
scheduler:
  type: "step"             # Options: "step", "exponential"
  step_size: 10000         # For step scheduler: reduce LR every N iterations
  gamma: 0.5               # Multiplicative factor for LR reduction

# Data configuration
data:
  batch_size: 4
  patch_size: 128
  insert_xy: 16
  num_workers: 4
  train_val_split: 0.8

# PSF configuration
psf:
  target_dx: 0.0313        # Target sampling interval in x (micrometers)
  target_dy: 0.0313        # Target sampling interval in y (micrometers)
  psf_dx: null             # PSF sampling interval in x (null = auto-detect)
  psf_dy: null             # PSF sampling interval in y (null = auto-detect)

# Trainer configuration
trainer:
  max_epochs: 100          # Extended training for better deconvolution performance
  precision: 32            # Training precision (16, 32, or 64)
  gradient_clip_val: 0.5   # Gradient clipping value
  accumulate_grad_batches: 1
  val_check_interval: 1.0  # Validate every epoch
  log_every_n_steps: 50

# Checkpoint configuration
checkpoint:
  monitor: "val_loss"      # Metric to monitor for checkpointing
  mode: "min"              # "min" for loss, "max" for accuracy
  save_top_k: 3            # Save top k models
  save_last: true          # Always save the last model

# Early stopping configuration
early_stopping:
  enabled: false           # Enable early stopping
  monitor: "val_loss"      # Metric to monitor
  mode: "min"              # "min" for loss, "max" for accuracy
  patience: 15             # Number of epochs without improvement
  min_delta: 0.001         # Minimum change to qualify as improvement

# Logging configuration
log_images: true           # Log sample images during training
log_every_n_epochs: 5      # How often to log images (more frequent for debugging two-stage)